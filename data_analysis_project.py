# -*- coding: utf-8 -*-
"""data analysis project

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MCFQe3qjpkfVaiyWdhaWuanK2rmTy8J4
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from scipy import stats
import warnings

warnings.filterwarnings('ignore')
sns.set(style="whitegrid")

# important libraies

df = pd.read_csv('fifa_eda_stats.csv')
df.head()

# loading the data

#basic explorations

df.shape
df.columns
df.info()
df.describe()

#examine the dataset's structure and basic statistics with the (info , describe) functions

#basic exploration
missing_values = df.isnull().sum()
missing_values[missing_values > 0]

#isnull().sum() function counts the number of missing values in each column, and we filter out only those columns where missing values are present.

#handle missing value
numeric_cols = df.select_dtypes(include=[np.number]).columns
df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].median())
df.isnull().sum().sum()

# handle missing values in numeric columns by filling them with the median value of each column

#handle duplicates
duplicates = df.duplicated().sum()
df.drop_duplicates(inplace=True)
df.duplicated().sum()
# dentifies and removes duplicate rows from the dataset using ( duplicated,sum ) functions & (drop_duplicates)

#univariant analysis
plt.figure(figsize=(10, 6))
sns.histplot(df['Overall'], bins=30, kde=True)
plt.title('Distribution of Overall Ratings')
plt.show()

#visualize the distribution of players' Overall ratings using a histogram

#multivariant analysis
print(df.columns)
relevant_features = ['Overall', 'Potential', 'Value', 'Wage', 'Age', 'Dribbling', 'Defending', 'Passing', 'Shooting']

numeric_relevant_features = [feature for feature in relevant_features if feature in df.columns and df[feature].dtype in [np.int64, np.float64]]


plt.figure(figsize=(14, 10))
sns.heatmap(df[numeric_relevant_features].corr(), annot=True, cmap='coolwarm', fmt=".2f", linewidths=0.5)
plt.title('Correlation Heatmap of Selected Features')
plt.show()

#creates a heatmap to visualize correlations between selected features

#dealing outliers
numeric_df = df.select_dtypes(include=[np.number])

Q1 = numeric_df.quantile(0.25)
Q3 = numeric_df.quantile(0.75)
IQR = Q3 - Q1

df_outliers_removed = numeric_df[~((numeric_df < (Q1 - 1.5 * IQR)) | (numeric_df > (Q3 + 1.5 * IQR))).any(axis=1)]
df.shape, df_outliers_removed.shape
#identify and remove outliers from numeric columns using the Interquartile Range (IQR) method

#one hot encoding
categorical_cols = df.select_dtypes(include=['object']).columns
df_encoded = pd.get_dummies(df, columns=categorical_cols)
df_encoded.head()
#Categorical columns are converted into numeric format using one-hot encoding

#feature scaling
scaler = StandardScaler()
scaled_features = scaler.fit_transform(df_encoded.select_dtypes(include=[np.number]))
df_scaled = pd.DataFrame(scaled_features, columns=df_encoded.select_dtypes(include=[np.number]).columns)
df_scaled.head()
#Numeric features are scaled to have a mean of 0 and a standard deviation of 1 using StandardScaler

# data splitting
X = df_scaled
y = df['Overall']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
X_train.shape, X_test.shape, y_train.shape, y_test.shape


#splits the data into the following
#X_train shape: (14565, 56)
#X_test shape: (3642, 56)
#y_train shape: (14565,)
#y_test shape: (3642,)